{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f71be-86f1-4cdf-a3eb-8a20b1c28c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=RuntimeWarning)\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob,os,sys\n",
    "from tqdm.auto import tqdm\n",
    "import proplot as plot\n",
    "import json,pickle\n",
    "import dask.array as da\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "sys.path.insert(1, '/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/TCGphy/2020_TC_CRF/dev/freddy0218/')\n",
    "from tools import derive_var,read_and_proc,preproc_noensemble\n",
    "from tools.mlr import mlr\n",
    "from tools.preprocess import do_eof,preproc_maria,preproc_haiyan\n",
    "%matplotlib inline\n",
    "plot.rc.update({'figure.facecolor':'w','axes.labelweight':'ultralight',\n",
    "                'tick.labelweight':'ultralight','gridminor.linestyle':'--','title.weight':'normal','linewidth':0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c4a88-53e2-43de-8604-5fc308d5d13e",
   "metadata": {},
   "source": [
    "#### Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c101f2-f43d-4299-856e-2746a1e62ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/'\n",
    "suffix = '_smooth_preproc_dict1b_g'\n",
    "a = [read_and_proc.depickle(path+'TCGphy/testML/output/haiyan/processed/uvwheat/'+'mem'+str(lime)+suffix)['u'].shape for lime in tqdm(range(1,21))]\n",
    "# divide experiments reference\n",
    "divider = np.asarray([aobj[0] for aobj in a]).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a85866d-708d-4dcc-bf90-67f7c7d13c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath='/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/TCGphy/testML/output/haiyan/processed/intermediate/'\n",
    "pcastore = read_and_proc.depickle(folderpath+'pca/PCA'+'_'+'dict2_g')\n",
    "dudvdwdth = read_and_proc.depickle(folderpath+'pca/dudvdwdth'+'_'+'dict2_g')\n",
    "haiyan_data = read_and_proc.depickle(folderpath+'pca/flatvar'+'_'+'dict2_g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626baf2-579d-49fb-9796-e77118f88965",
   "metadata": {},
   "source": [
    "#### Check components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4fda0a-e1c5-4e15-80cd-cf0a32c876bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diff(arrayin=None,delta=None,axis=None,LT=1):\n",
    "    result = []\n",
    "    if axis==0:\n",
    "        for i in range(0,arrayin.shape[axis]-LT):\n",
    "            temp = (arrayin[i+LT,:]-arrayin[i,:])/(LT*delta)\n",
    "            result.append(temp)\n",
    "        return np.asarray(result)\n",
    "    \n",
    "class datacheck:\n",
    "    def __init__(self,pcadict=None,flatvardict=False,divider=None):\n",
    "        #self.folderpath=folderpath\n",
    "        self.pcadict = pcadict\n",
    "        self.ctlflatvar = flatvardict\n",
    "        self.divider = divider\n",
    "        \n",
    "    def _back_to_exp(self,timeseries=None,divider=None):\n",
    "        printout = [timeseries[0:divider[0],:]]\n",
    "        for i in range(1,19):\n",
    "            printout.append(timeseries[divider[i-1]:divider[i],:])\n",
    "        printout.append(timeseries[divider[-2]:,:])\n",
    "        return printout\n",
    "    \n",
    "    def dudvdwVAR(self,dudvdw=None,vartest='w'):\n",
    "        #dudvdw = read_and_proc.depickle(self.folderpath+dudvdwpath)\n",
    "        timeseries = self.pcadict[vartest].transform(self.ctlflatvar[vartest])\n",
    "        left_dot = [forward_diff(obj,60*60,0,1) for obj in self._back_to_exp(timeseries,self.divider)]\n",
    "        left_dott = np.concatenate([obj for obj in left_dot],axis=0)\n",
    "        for i in np.linspace(0,90,46):\n",
    "            tempobj = np.dot(left_dott[:,0:int(i)],(self.pcadict[vartest].components_[0:int(i)]))\n",
    "            print((np.var(tempobj)/np.var(dudvdw['d'+str(vartest)])).data)\n",
    "        #TESTu_var = [np.var(obj)/np.var(dudvdw['d'+str(vartest)]) for obj in TESTu]\n",
    "        del left_dot,left_dott\n",
    "        gc.collect()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a22200e-7ca5-4022-94a6-b153daf2fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPCA_projection_sen(pca_dict=None,varname=None,toproj_flatvar=None,orig_flatvar=None):\n",
    "    pca_orig = pca_dict[varname].transform(orig_flatvar)\n",
    "    if pca_dict[varname].mean_ is not None:\n",
    "        orig_mean = pca_dict[varname].mean_\n",
    "    projvar_transformed = np.dot(toproj_flatvar-np.nanmean(toproj_flatvar,axis=0),pca_dict[varname].components_.T)\n",
    "    del orig_mean\n",
    "    gc.collect()\n",
    "    return pca_orig, projvar_transformed\n",
    "\n",
    "class input_output:\n",
    "    def __init__(self,PCAdict=None,folderpath=None,ts_varname=None,nummem=None):\n",
    "        self.PCAdict = PCAdict\n",
    "        self.varname=ts_varname\n",
    "        self.nummem = nummem # u: 36 (40% variability in du), v:16/32 (40% dv var;50%), w:44 (40% dw var)\n",
    "    \n",
    "    ###################################################################################################################################################\n",
    "    # Produce time series\n",
    "    ###################################################################################################################################################    \n",
    "    def produce_timeseries(self,flatvar=None):\n",
    "        ts_dict = {}\n",
    "        for indx,obj in tqdm(enumerate(self.varname)):\n",
    "            ts_dict[obj] = self.PCAdict[obj].transform(flatvar[obj].data)[:,0:self.nummem[indx]]\n",
    "        return ts_dict\n",
    "    \n",
    "    def produce_Qsentimeseries(self,senvar_name=None,refvar_name='rad',numQ=None,flatvar=None,senflatvar=None):\n",
    "        ts_dict = {}\n",
    "        temp = [myPCA_projection_sen(pca_dict=self.PCAdict,varname=refvar_name,toproj_flatvar=senflatvar[obj].data,orig_flatvar=flatvar[refvar_name].data)[1][:,0:numQ] for indx,obj in enumerate(senvar_name)]\n",
    "        return dict(zip(senvar_name,temp))\n",
    "    \n",
    "    def normalize_timeseries(self,timeseries=None):\n",
    "        assert timeseries['u'].shape[-1]==26,\"var shape error\"\n",
    "        ts_dict = {}\n",
    "        for indx,obj in tqdm(enumerate(self.varname)):\n",
    "            ts_dict[obj] = (timeseries[obj]-np.nanmean(timeseries[obj],axis=0))/np.nanstd(timeseries[obj],axis=0)\n",
    "        return ts_dict\n",
    "    \n",
    "    def normalize_timeseries_decomp(self,sentimeseries=None,reftimeseries=None,senvarnames=None,refvarname='rad'):\n",
    "        ts_dict = {}\n",
    "        meanstd_dict = {}\n",
    "        for indx,obj in tqdm(enumerate(senvarnames)):\n",
    "            tempf = -np.mean(sentimeseries[obj],axis=0)/np.std(reftimeseries[refvarname],axis=0)\n",
    "            ts_dict[obj] = (sentimeseries[obj]-np.mean(sentimeseries[obj],axis=0))/np.std(reftimeseries[refvarname],axis=0)\n",
    "            meanstd_dict[obj] = np.broadcast_to(tempf, (sentimeseries[obj].shape[0], sentimeseries[obj].shape[1]))\n",
    "        return ts_dict,meanstd_dict\n",
    "    \n",
    "    ###################################################################################################################################################\n",
    "    # Produce Input Dataset\n",
    "    ###################################################################################################################################################      \n",
    "    def _back_to_exp(self,timeseries=None,divider=None):\n",
    "        printout = [timeseries[0:divider[0],:]]\n",
    "        for i in range(1,19):\n",
    "            printout.append(timeseries[divider[i-1]:divider[i],:])\n",
    "        printout.append(timeseries[divider[-2]:,:])\n",
    "        return printout\n",
    "    \n",
    "    def back_to_exp(self,inputlong=None,divider=None,senvarname=None):\n",
    "        ts_dict = {}\n",
    "        if senvarname is None:\n",
    "            for indx,obj in tqdm(enumerate(self.varname)):\n",
    "                ts_dict[obj] = self._back_to_exp(inputlong[obj],divider)\n",
    "        else:\n",
    "            for indx,obj in tqdm(enumerate(senvarname)):\n",
    "                ts_dict[obj] = self._back_to_exp(inputlong[obj],divider)            \n",
    "        return ts_dict\n",
    "    \n",
    "    def train_valid_test(self,expvarlist=None,validindex=None,testindex=None,concat='Yes'):\n",
    "        X_valid, X_test = [expvarlist[i] for i in validindex], [expvarlist[i] for i in testindex]\n",
    "        X_traint = expvarlist.copy()\n",
    "        popindex = validindex+testindex\n",
    "        #[X_train.pop(i) for i in validindex]\n",
    "        #[X_train.pop(i) for i in testindex]\n",
    "        X_train = [X_traint[i] for i in range(len(X_traint)) if i not in popindex]\n",
    "        assert len(X_train)==16, 'wrong train-valid-test separation!'\n",
    "        if concat=='Yes':\n",
    "            return np.concatenate([X_train[i] for i in range(len(X_train))],axis=0), np.concatenate([X_valid[i] for i in range(len(X_valid))],axis=0), np.concatenate([X_test[i] for i in range(len(X_test))],axis=0)\n",
    "        else:\n",
    "            return X_train, X_valid, X_test\n",
    "    \n",
    "    def make_X(self,expvarlist=None,varwant=None,validindex=None,testindex=None,concat='Yes'):\n",
    "        trainlist,validlist,testlist = [],[],[]\n",
    "        for obj in varwant:\n",
    "            test1,test2,test3 = self.train_valid_test(exp_pca_norml[obj],validindex,testindex,'Yes')\n",
    "            trainlist.append(test1)\n",
    "            validlist.append(test2)\n",
    "            testlist.append(test3)\n",
    "        return np.concatenate([trainlist[i] for i in range(len(trainlist))],axis=1), np.concatenate([validlist[i] for i in range(len(validlist))],axis=1), np.concatenate([testlist[i] for i in range(len(testlist))],axis=1)\n",
    "    \n",
    "    ###################################################################################################################################################\n",
    "    # Produce Output Dataset\n",
    "    ###################################################################################################################################################\n",
    "    def get_time_diff_terms(self,inputvar=None,LT=None):\n",
    "        def _get_time_diff(array=None,timedelta=60*60,LT=None):\n",
    "            store = []\n",
    "            for exp in array:\n",
    "                a = forward_diff(exp,timedelta,0,LT)\n",
    "                if a.shape[0]>0:\n",
    "                    azero = np.zeros((LT,exp.shape[-1]))\n",
    "                    store.append(np.concatenate((a,azero),axis=0))\n",
    "                else:\n",
    "                    store.append(np.zeros((exp.shape[0],exp.shape[-1])))\n",
    "            return store\n",
    "        \n",
    "        storedict = {}\n",
    "        for wantvar in ['u','v','w','theta']:\n",
    "            storedict[wantvar] = _get_time_diff(array=inputvar[wantvar],LT=LT)\n",
    "        return storedict\n",
    "    \n",
    "    def make_Y(self,inputdict=None,LDT=None,validindex=[1,6],testindex=[2,12]):\n",
    "        def _make_Y(inputt=None):\n",
    "            varTRAIN,varVALID,varTEST = [],[],[]\n",
    "            for varobj in ['u','v','w','theta']:\n",
    "                test1,test2,test3 = self.train_valid_test(expvarlist=inputt[varobj],validindex=validindex,testindex=testindex,concat='Yes')\n",
    "                varTRAIN.append(test1)\n",
    "                varVALID.append(test2)\n",
    "                varTEST.append(test3)\n",
    "            return np.concatenate([varTRAIN[i] for i in range(len(varTRAIN))],axis=1), np.concatenate([varVALID[i] for i in range(len(varVALID))],axis=1), np.concatenate([varTEST[i] for i in range(len(varTEST))],axis=1)\n",
    "        test = [self.get_time_diff_terms(inputdict,int(LDTobj)) for LDTobj in LDT]\n",
    "        return [_make_Y(timediffobj) for timediffobj in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aefcfd14-bc7b-4c07-a98e-f374f6e3a4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 0, 10, 9, 3, 6, 6, 4, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools.mlr import mlr,maria_IO,haiyan_IO\n",
    "#datacheck(pcastore,haiyan_data,divider).dudvdwVAR(dudvdwdth,'w')\n",
    "[np.abs(pcastore[obj].explained_variance_ratio_.cumsum()-0.91).argmin() for obj in ['u','v','w','qv','theta','heatsum','hdia','rad','ir']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdcb33b1-37dc-4745-9e4a-f21bda7871a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rand_nums = random.sample(range(1,20),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e6d9bc6-60ee-46b1-ac75-455dc47fc494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c04550d16740c89f955c8a89744e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5ed6abc84042a9997f683a5437bfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7f9d5a14e44c1a9938bc2bd48ad34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1563ca7eb42749ed8e3e47a2b1a3b9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65774aa94da419dbc591bfc52a71529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nummem=[26,18,48,9,14,6,6,4,3]\n",
    "haiyan_pca_nonorml = input_output( pcastore, None, ['u', 'v', 'w', 'qv','theta', 'heatsum', 'hdia', 'rad', 'ir'], nummem ).produce_timeseries( haiyan_data )\n",
    "haiyan_pca_norml = input_output( pcastore, None, ['u', 'v', 'w', 'qv','theta', 'heatsum', 'hdia', 'rad', 'ir'], nummem ).normalize_timeseries( haiyan_pca_nonorml )\n",
    "\n",
    "exp_pca_norml = input_output( pcastore, None, ['u', 'v', 'w', 'qv','theta', 'heatsum', 'hdia', 'rad', 'ir'], nummem ).back_to_exp( haiyan_pca_norml, divider ) \n",
    "exp_pca_nonorml = input_output( pcastore, None, ['u', 'v', 'w', 'qv','theta', 'heatsum', 'hdia', 'rad', 'ir'], nummem ).back_to_exp( haiyan_pca_nonorml, divider ) \n",
    "\n",
    "EXP = [['hdia','rad'],['theta','hdia','rad'],['u','v','w','theta','hdia','rad'],['u','v','theta','hdia','rad'],['u','v'],['u','v','w'],['u','v','hdia','rad'],['u','v','w','hdia','rad'],['u','v','w','theta','qv','hdia','rad']]\n",
    "storename = ['dt','dtth','dtthuvw','dtthuv','uv','uvw','dtuv','dtuvw','dtthuvwqv']\n",
    "import random\n",
    "\n",
    "for i in tqdm(range(33)):\n",
    "    rand_nums = random.sample(range(1,20),4)\n",
    "    X_traindict,X_validdict,X_testdict = {},{},{}\n",
    "    for ind,obj in enumerate(storename):\n",
    "        test1,test2,test3 = (input_output(pcastore,None,['u','v','w','qv','theta','heatsum','hdia','rad','ir'],None).make_X(exp_pca_norml,EXP[ind],rand_nums[0:2],rand_nums[2:4],'Yes'))\n",
    "        X_traindict[obj] = test1\n",
    "        X_validdict[obj] = test2\n",
    "        X_testdict[obj] = test3\n",
    "    folderpath='/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/TCGphy/testML/output/haiyan/processed/intermediate/'\n",
    "    read_and_proc.save_to_pickle(folderpath+'pca/X/random/Xtrain'+str(rand_nums[0])+'_'+str(rand_nums[1])+'_'+str(rand_nums[2])+'_'+str(rand_nums[3])+'_'+'dict_g',X_traindict,'PICKLE')\n",
    "    read_and_proc.save_to_pickle(folderpath+'pca/X/random/Xvalid'+str(rand_nums[0])+'_'+str(rand_nums[1])+'_'+str(rand_nums[2])+'_'+str(rand_nums[3])+'_'+'dict_g',X_validdict,'PICKLE')\n",
    "    read_and_proc.save_to_pickle(folderpath+'pca/X/random/Xtest'+str(rand_nums[0])+'_'+str(rand_nums[1])+'_'+str(rand_nums[2])+'_'+str(rand_nums[3])+'_'+'dict_g',X_testdict,'PICKLE')\n",
    "    \n",
    "    y_all = (input_output(pcastore,None,['u','v','w','qv','theta','heatsum','hdia','rad','ir'],nummem).make_Y(exp_pca_nonorml,np.linspace(0,35,36)+1,rand_nums[0:2],rand_nums[2:4]))\n",
    "    folderpath='/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/TCGphy/testML/output/haiyan/processed/intermediate/'\n",
    "    read_and_proc.save_to_pickle(folderpath+'pca/y/random/allY'+str(rand_nums[0])+'_'+str(rand_nums[1])+'_'+str(rand_nums[2])+'_'+str(rand_nums[3])+'_'+'dict_g',y_all,'PICKLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acaa6c1-4b67-4430-9a19-cb283452d03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
